{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d36d5c6-8246-46b6-b53a-7dc9ce28cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"v20251219\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using JSON\n",
    "using ArchGDAL\n",
    "using Proj\n",
    "using Rasters\n",
    "using Base.Threads\n",
    "using JLD2\n",
    "using Lux\n",
    "using LuxCore\n",
    "using EasyHybrid\n",
    "using Optimisers\n",
    "using Statistics\n",
    "using Plots\n",
    "include(\"helpers.jl\")\n",
    "using .Helpers\n",
    "version = \"v20251219\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3f9e4-8e69-4244-963f-c79d6ea7448f",
   "metadata": {},
   "source": [
    "## Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc65228-5035-48a6-adec-f1d497c57ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformation unknown\n",
       "    source: ETRS89-extended / LAEA Europe\n",
       "    target: WGS 84\n",
       "    direction: forward\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load cov list\n",
    "cov = JSON.parsefile(\"./cov_path_full.json\");\n",
    "pnames = collect(keys(cov))\n",
    "paths = collect(values(cov))\n",
    "\n",
    "res_m = 100 # meters\n",
    "bboxmine = (8.956051,51.815757,10.450192,53.154421) # examine area in northern DE, suggested by Bernhard\n",
    "bbox3035 = convert_bbox_wgs84_to_3035(bboxmine);\n",
    "xs, ys = make_grid_3035(bbox3035, res_m);\n",
    "\n",
    "tf = Proj.Transformation(\"EPSG:3035\", \"EPSG:4326\") # because it's always 4326, so we do it in lazy way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9df5666b-74eb-4638-92e2-96b04815b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia threads: 96\n",
      "length: 362\n"
     ]
    }
   ],
   "source": [
    "tnames = pnames#[10:30]\n",
    "tpaths = paths#[10:30]  \n",
    "println(\"Julia threads: \", Threads.nthreads())\n",
    "println(\"length: \", length(tpaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e6bb6-3265-45a2-b006-c7a096c9b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk - 0\n",
      " 83.943894 seconds (321.11 M allocations: 9.654 GiB, 2.31% gc time, 68.47% compilation time)\n",
      "chunk - 1\n",
      " 83.502818 seconds (331.53 M allocations: 9.762 GiB, 2.35% gc time, 0.50% compilation time)\n",
      "chunk - 2\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 40\n",
    "out = Vector{Any}(undef, length(tpaths))\n",
    "m = Int[]\n",
    "lock_m = ReentrantLock()\n",
    "iii = 0\n",
    "for chunk in Iterators.partition(eachindex(tpaths), chunk_size)\n",
    "    println(\"chunk - $(iii)\")\n",
    "\n",
    "    @time @threads for i in chunk\n",
    "        try\n",
    "            out[i] = sample_tiff_onto_grid(tpaths[i], xs, ys, tf)\n",
    "        catch\n",
    "            lock(lock_m) do\n",
    "                push!(m, i)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    iii = iii + 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7f039-fc6a-4855-8542-35e72319661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame()\n",
    "nx = length(xs)\n",
    "ny = length(ys)\n",
    "df.x3035 = repeat(xs, inner=ny)\n",
    "df.y3035 = repeat(ys, outer=nx)\n",
    "for i in eachindex(tnames)\n",
    "    df[!, Symbol(tnames[i])] = out[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914efeb0-0dd4-418c-9143-d421c88c63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"overlaid_$(res_m)m_$(version).csv\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6102fd-7194-4959-a821-16b93e7258ec",
   "metadata": {},
   "source": [
    "## prepare the data as how it's processed before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7696b98-7558-4a2c-aace-e021be191843",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames\n",
    "# prepare data\n",
    "# load in preprocessed data to get predictors\n",
    "datafile = \"/mnt/tupi/HybridModeling/EasyDensity.jl/data/lucas_preprocessed_v20251125.csv\"\n",
    "oridf = CSV.read(datafile, DataFrame; normalizenames=true)\n",
    "predictors = Symbol.(names(oridf))[18:end-6]; # CHECK EVERY TIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04304891-3873-4d3a-8580-1e57ac747382",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "# ? move the `csv` file into the `BulkDSOC/data` folder (create folder)\n",
    "df_o = CSV.read(\"/mnt/tupi/HybridModeling/EasyDensity.jl/data/lucas_overlaid.csv\", DataFrame, normalizenames=true);\n",
    "println(size(df_o));\n",
    "\n",
    "############################\n",
    "###### clean targets #######\n",
    "############################\n",
    "\n",
    "# filter horizon depth = 10 cm\n",
    "df_o = df_o[df_o.hzn_dep .== 10, :];\n",
    "select!(df_o, Not(:hzn_dep));\n",
    "println(size(df_o))\n",
    "\n",
    "# identify noise time supervise\n",
    "gdf = groupby(df_o, :id);\n",
    "df_o.maxdiff = fill(0.0, nrow(df_o));  # initialize noise column\n",
    "# compute max abs difference of SOCconc per id\n",
    "for sub in groupby(df_o, :id)\n",
    "    soc = sort(sub.soc)\n",
    "\n",
    "    if length(soc) < 2\n",
    "        maxdiff = -1\n",
    "    else\n",
    "        maxdiff = maximum(abs.(diff(soc)))\n",
    "    end\n",
    "\n",
    "    df_o[df_o.id .== sub.id[1], :maxdiff] .= maxdiff\n",
    "    \n",
    "end\n",
    "println(size(df_o))\n",
    "df_o = df_o[df_o.maxdiff .<= 50, :];\n",
    "println(size(df_o))\n",
    "\n",
    "# coords = collect(zip(df_o.lat, df_o.lon));\n",
    "\n",
    "########################\n",
    "###### clean cov #######\n",
    "########################\n",
    "# t clean covariates\n",
    "names_cov = Symbol.(names(df_o))[18:end-1];\n",
    "\n",
    "# Fix soilsuite and cropland extent columns\n",
    "for col in names_cov\n",
    "    if occursin(\"_soilsuite_\", String(col))\n",
    "        df_o[!, col] = replace(df_o[!, col], missing => 0)\n",
    "    elseif occursin(\"cropland_extent_\", String(col))\n",
    "        df_o[!, col] = replace(df_o[!, col], missing => 0)\n",
    "        df_o[!, col] .= ifelse.(df_o[!, col] .> 0, 1, 0)\n",
    "    end\n",
    "end\n",
    "\n",
    "# rm missing values: 1. >5%, drop col; 2. <=5%, drop row\n",
    "cols_to_drop_row = Symbol[];\n",
    "cols_to_drop_col = Symbol[];\n",
    "for col in names_cov\n",
    "    n_missing = count(ismissing, df_o[!, col])\n",
    "    frac_missing = n_missing / nrow(df_o)\n",
    "    if frac_missing > 0.05\n",
    "        println(n_missing, \" \", col)\n",
    "        select!(df_o, Not(col))  # drop the column\n",
    "        push!(cols_to_drop_col, col)  \n",
    "    elseif n_missing > 0\n",
    "        # println(n_missing, \" \", col)\n",
    "        push!(cols_to_drop_row, col)  # collect column name\n",
    "    end\n",
    "\n",
    "    if occursin(\"CHELSA_kg\", String(col)) \n",
    "        push!(cols_to_drop_col, col) \n",
    "        select!(df_o, Not(col))  # rm kg catagorical col\n",
    "    end \n",
    "end\n",
    "\n",
    "names_cov = filter(x -> !(x in cols_to_drop_col), names_cov) # remove cols-to-drop from names_cov\n",
    "if !isempty(cols_to_drop_row) \n",
    "    df_o = subset(df_o, cols_to_drop_row .=> ByRow(!ismissing)) # drop rows with missing values in cols_to_drop_row\n",
    "end\n",
    "println(size(df_o))\n",
    "\n",
    "cols_to_drop_col = Symbol[] \n",
    "for col in names_cov\n",
    "    if std(df_o[:,col])==0\n",
    "        push!(cols_to_drop_col, col)  # rm constant col (std==0)\n",
    "        select!(df_o, Not(col))\n",
    "    end\n",
    "end\n",
    "names_cov = filter(x -> !(x in cols_to_drop_col), names_cov) # remove cols-to-drop from names_cov\n",
    "println(size(df_o))\n",
    "\n",
    "# for col in names_cov # to check covairate distribution\n",
    "#     println(string(col)[1:10], ' ', round(std(df[:, col]); digits=2), ' ', round(mean(df[:, col]); digits=2))\n",
    "# end\n",
    "\n",
    "# # Normalize covariates by (x-mean) / std\n",
    "means = map(c -> mean(skipmissing(df_o[!, c])), predictors)\n",
    "stds  = map(c -> std(skipmissing(df_o[!, c])), predictors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d203830-347a-4823-aae4-ae3371898aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply the normalizations to new training data\n",
    "# get the overlaid data\n",
    "version = \"v20251219\"\n",
    "df = CSV.read(\"overlaid_$(version).csv\", DataFrame)\n",
    "\n",
    "# mend crop and soil suite layers\n",
    "for col in predictors\n",
    "    if occursin(\"_soilsuite_\", String(col))\n",
    "        df[!, col] = replace(df[!, col], missing => 0)\n",
    "    elseif occursin(\"cropland_extent_\", String(col))\n",
    "        df[!, col] = replace(df[!, col], missing => 0)\n",
    "        df[!, col] .= ifelse.(df[!, col] .> 0, 1, 0)\n",
    "    end\n",
    "end\n",
    "\n",
    "for (i, col) in enumerate(predictors)\n",
    "    df[!, col] = (Float64.(df[!, col]) .- means[i]) ./ stds[i]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8436b3d-f18f-44e7-baa7-23f77b10a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(x) = filter(!isnan, skipmissing(x))\n",
    "\n",
    "rows = Vector{NamedTuple}()\n",
    "\n",
    "for col in predictors\n",
    "    v_o = clean(oridf[!, col])\n",
    "    v_d = clean(df[!, col])\n",
    "\n",
    "    push!(rows, (\n",
    "        variable   = String(col),\n",
    "        q05_oridf  = quantile(v_o, 0.05),\n",
    "        q05_df     = quantile(v_d, 0.05),\n",
    "        q50_oridf  = quantile(v_o, 0.50),\n",
    "        q50_df     = quantile(v_d, 0.50),\n",
    "        q95_oridf  = quantile(v_o, 0.95),\n",
    "        q95_df     = quantile(v_d, 0.95)\n",
    "    ))\n",
    "end\n",
    "\n",
    "qt = DataFrame(rows)\n",
    "CSV.write(\"predictor_quantiles_check.csv\", qt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838ad24-a17b-4a79-8ff0-dd24cadb3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"production_preprocessed_$(res_m)m_$(version).csv\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7c80b-3585-4901-bd23-a9d9f78da2ea",
   "metadata": {},
   "source": [
    "## load model, retrain on all data and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b681a05-1754-4b58-977c-0bbc77b967d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jldfile = \"/mnt/tupi/HybridModeling/EasyDensity.jl/model/best_model_03_hybridNN_config105_fold4.jld2\"\n",
    "# hm = load_last_epoch(jldfile);\n",
    "# ps = hm[1]\n",
    "# st = hm[2]\n",
    "\n",
    "# prepare data\n",
    "# load in preprocessed data to get predictors\n",
    "datafile = \"/mnt/tupi/HybridModeling/EasyDensity.jl/data/lucas_preprocessed_v20251125.csv\"\n",
    "oridf = CSV.read(datafile, DataFrame; normalizenames=true)\n",
    "predictors = Symbol.(names(oridf))[18:end-6]; # CHECK EVERY TIME \n",
    "\n",
    "# parameters\n",
    "scalers = Dict(\n",
    "    :SOCconc   => 0.151, # g/kg, log(x+1)*0.151\n",
    "    :CF        => 0.263, # percent, log(x+1)*0.263\n",
    "    :BD        => 0.529, # g/cm3, x*0.529\n",
    "    :SOCdensity => 0.167, # kg/m3, log(x)*0.167\n",
    ");\n",
    "\n",
    "parameters = (\n",
    "    SOCconc = (0.01f0, 0.0f0, 1.0f0),   # fraction\n",
    "    CF      = (0.15f0, 0.0f0, 1.0f0),   # fraction,\n",
    "    oBD     = (0.20f0, 0.05f0, 0.40f0),  # also NN learnt, g/cm3\n",
    "    mBD     = (1.20f0, 0.75f0, 2.0f0),  # NN leanrt\n",
    ")\n",
    "neural_param_names = [:SOCconc, :CF, :mBD, :oBD]\n",
    "forcing = Symbol[]\n",
    "targets = [:BD, :SOCconc, :SOCdensity, :CF]   \n",
    "\n",
    "hmb = constructHybridModel(\n",
    "    predictors,\n",
    "    forcing,\n",
    "    targets,\n",
    "    SOCD_model,\n",
    "    parameters,\n",
    "    neural_param_names,\n",
    "    [];\n",
    "    hidden_layers = [256, 128, 64, 32],\n",
    "    activation = gelu,\n",
    "    scale_nn_outputs = true,\n",
    "    input_batchnorm = false,\n",
    "    start_from_default = true\n",
    ");\n",
    "\n",
    "rlt = train(\n",
    "    hmb, oridf, ();\n",
    "    nepochs = 200,\n",
    "    batchsize = bs,\n",
    "    opt = AdamW(lr),\n",
    "    training_loss = :mse,\n",
    "    loss_types = [:mse, :r2],\n",
    "    shuffleobs = true,\n",
    "    file_name = \"prod_SiNN.jld2\",\n",
    "    random_seed = 42,\n",
    "    patience = 15,\n",
    "    yscale = identity,\n",
    "    monitor_names = [:oBD, :mBD],\n",
    "    agg = mean,\n",
    "    return_model = :best,\n",
    "    show_progress = false,\n",
    "    plotting = false,\n",
    "    hybrid_name = \"prod_SiNN\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d078a05-f1e3-412f-b34c-384b1b294990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the preprocessed overlaid data\n",
    "df = CSV.read(\"production_preprocessed_$(res_m)m_$(version).csv\", DataFrame)\n",
    "x_test = to_keyedArray(Float32.(df[!, predictors]));\n",
    "\n",
    "ps, st = rlt.ps, rlt.st\n",
    "ŷ_test, st_test = best_model(x_test, ps, LuxCore.testmode(st))\n",
    "\n",
    "# prediction\n",
    "for var in [:BD, :SOCconc, :CF, :SOCdensity, :oBD, :mBD]\n",
    "    if hasproperty(ŷ_test, var)\n",
    "        val = getproperty(ŷ_test, var)\n",
    "\n",
    "        if val isa AbstractVector && length(val) == nrow(df)\n",
    "            df[!, Symbol(\"pred_\", var)] = val # per row\n",
    "\n",
    "        elseif (val isa Number) || (val isa AbstractVector && length(val) == 1)\n",
    "            df[!, Symbol(\"pred_\", var)] = fill(Float32(val isa AbstractVector ? first(val) : val), nrow(df))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# inverse SOC concentration (g/kg)\n",
    "df[!, :soc] = exp.(df[!, :pred_SOCconc] ./ scalers[:SOCconc]) .- 1;\n",
    "# inverse CF (fraction, NOT percent)\n",
    "df[!, :cf] = (exp.(df[!, :pred_CF] ./ scalers[:CF]) .- 1) ./ 100;\n",
    "# inverse bulk density (g/cm3)\n",
    "df[!, :bd] = df[!, :pred_BD] ./ scalers[:BD];\n",
    "# inverse SOC density (kg/m3)\n",
    "df[!, :ocd] = exp.(df[!, :pred_SOCdensity] ./ scalers[:SOCdensity]);\n",
    "\n",
    "println(size(df))\n",
    "save_col = [\n",
    "    :x3035, :y3035,\n",
    "    :pred_SOCconc, :pred_CF, :pred_BD,\n",
    "    :pred_SOCdensity,\n",
    "    :soc, :ocd, :bd, :cf\n",
    "]\n",
    "\n",
    "CSV.write(\"out_$(res_m)m_$(version).csv\", df[:, save_col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8bd30-d0d2-4c90-bc7a-f47cf1a0495e",
   "metadata": {},
   "source": [
    "## check predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c6f1b1-1318-442d-bbc7-d363ad0ac12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using Plots\n",
    "for col in [\"pred_BD\", \"pred_SOCconc\", \"pred_CF\", \"pred_SOCdensity\"]\n",
    "\n",
    "    vals = df[:, col]\n",
    "\n",
    "    # 有效值（非 missing 且非 NaN）\n",
    "    valid_vals = filter(x -> !ismissing(x) && !isnan(x), vals)\n",
    "\n",
    "    n_valid = length(valid_vals)\n",
    "    vmin = minimum(valid_vals)\n",
    "    vmax = maximum(valid_vals)\n",
    "\n",
    "    println(\"Variable: $col\")\n",
    "    println(\"  Valid count = $n_valid\")\n",
    "    println(\"  Min = $vmin\")\n",
    "    println(\"  Max = $vmax\\n\")\n",
    "\n",
    "    histogram(\n",
    "        vals;\n",
    "        bins = 50,\n",
    "        xlabel = col,\n",
    "        ylabel = \"Frequency\",\n",
    "        title = \"Histogram of $col\",\n",
    "        lw = 1,\n",
    "        legend = false\n",
    "    )\n",
    "    display(current())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa70eaba-f69c-4bae-b382-be11791bdc0e",
   "metadata": {},
   "source": [
    "## save as tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d5b45-8887-4549-9b8c-2720a31dcca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in [:bd, :soc, :cf, :ocd, :pred_oBD, :pred_mBD]\n",
    "    write_geotiff_from_grid(\n",
    "        df,\n",
    "        xs,\n",
    "        ys,\n",
    "        var,\n",
    "        \"pred_$(var).tif\"\n",
    "    )\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
