{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d36d5c6-8246-46b6-b53a-7dc9ce28cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"v20251219\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV, DataFrames\n",
    "version = \"v20251219\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3f9e4-8e69-4244-963f-c79d6ea7448f",
   "metadata": {},
   "source": [
    "## Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc65228-5035-48a6-adec-f1d497c57ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformation unknown\n",
       "    source: ETRS89-extended / LAEA Europe\n",
       "    target: WGS 84\n",
       "    direction: forward\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load cov list\n",
    "using JSON\n",
    "using ArchGDAL\n",
    "using Proj\n",
    "using DataFrames\n",
    "using Rasters\n",
    "using Base.Threads\n",
    "\n",
    "cov = JSON.parsefile(\"./cov_path_full.json\");\n",
    "pnames = collect(keys(cov))\n",
    "paths = collect(values(cov))\n",
    "\n",
    "using Proj, ArchGDAL\n",
    "\n",
    "function make_grid_3035(bbox, res_m)\n",
    "    # bbox = (xmin, ymin, xmax, ymax) in EPSG:3035\n",
    "\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "\n",
    "    xs = collect(xmin:res_m:xmax)\n",
    "    ys = collect(ymax:-res_m:ymin)  # north → south\n",
    "\n",
    "    return xs, ys\n",
    "end\n",
    "\n",
    "function sample_tiff_onto_grid(tif_path, xs, ys, tf)\n",
    "    ArchGDAL.read(\"/vsicurl/\" * tif_path) do ds\n",
    "\n",
    "        # 1. 检查 raster 的 CRS（WKT）\n",
    "        tiff_wkt = ArchGDAL.getproj(ds)\n",
    "        same_crs = occursin(\"3035\", lowercase(tiff_wkt))\n",
    "        \n",
    "        # 2. GeoTransform\n",
    "        band = ArchGDAL.getband(ds, 1)\n",
    "        gt = ArchGDAL.getgeotransform(ds)\n",
    "        x0, dx, _, y0, _, dy = gt  # 注意 dx, dy 可能是负的\n",
    "\n",
    "        nx = length(xs)\n",
    "        ny = length(ys)\n",
    "        arr = Matrix{Float32}(undef, ny, nx)\n",
    "\n",
    "        @inbounds for i in 1:ny\n",
    "            for j in 1:nx\n",
    "                x3035 = xs[j]\n",
    "                y3035 = ys[i]\n",
    "\n",
    "                # 3. 如果 raster 不是 3035，把 (x3035,y3035) 转成 raster CRS 坐标\n",
    "                if same_crs\n",
    "                    xr = x3035\n",
    "                    yr = y3035\n",
    "                else\n",
    "                    # 关键：这里是 tf(3035 -> raster)\n",
    "                    yr, xr = tf(y3035, x3035)\n",
    "                end\n",
    "\n",
    "                # 4. 用 raster CRS 坐标变成 raster 像素 index（1-based）\n",
    "                px = Int(round((xr - x0) / dx)) + 1\n",
    "                py = Int(round((yr - y0) / dy)) + 1\n",
    "\n",
    "                if px < 1 || py < 1 ||\n",
    "                   px > ArchGDAL.width(ds) || py > ArchGDAL.height(ds)\n",
    "                    arr[i, j] = NaN32\n",
    "                    continue\n",
    "                end\n",
    "\n",
    "                val = ArchGDAL.read(band, py:py, px:px)\n",
    "                arr[i, j] = Float32(val[1])\n",
    "            end\n",
    "        end\n",
    "\n",
    "        return vec(arr)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function convert_bbox_wgs84_to_3035(bbox_wgs84)\n",
    "    xmin_lon, ymin_lat, xmax_lon, ymax_lat = bbox_wgs84\n",
    "    \n",
    "    tf = Proj.Transformation(\"EPSG:4326\", \"EPSG:3035\")\n",
    "\n",
    "    y1, x1 = tf(ymin_lat, xmin_lon)\n",
    "    y2, x2 = tf(ymax_lat, xmin_lon)\n",
    "    y3, x3 = tf(ymin_lat, xmax_lon)\n",
    "    y4, x4 = tf(ymax_lat, xmax_lon)\n",
    "\n",
    "    xs = (x1, x2, x3, x4)\n",
    "    ys = (y1, y2, y3, y4)\n",
    "\n",
    "    return (minimum(xs), minimum(ys), maximum(xs), maximum(ys))\n",
    "end\n",
    "\n",
    "\n",
    "res_m = 1000 # meters\n",
    "bboxmine = (8.956051,51.815757,10.450192,53.154421) # examine area in northern DE, suggested by Bernhard\n",
    "bbox3035 = convert_bbox_wgs84_to_3035(bboxmine);\n",
    "xs, ys = make_grid_3035(bbox3035, res_m);\n",
    "\n",
    "tf = Proj.Transformation(\"EPSG:3035\", \"EPSG:4326\") # because it's always 4326, so we do it in lazy way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df5666b-74eb-4638-92e2-96b04815b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia threads: 96\n",
      "length: 362\n"
     ]
    }
   ],
   "source": [
    "tnames = pnames#[270:360]\n",
    "tpaths = paths#[270:360]  \n",
    "println(\"Julia threads: \", Threads.nthreads())\n",
    "println(\"length: \", length(tpaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567e6bb6-3265-45a2-b006-c7a096c9b398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk - 0\n",
      "  2.562889 seconds (4.68 M allocations: 171.700 MiB, 4.54% gc time, 1725.23% compilation time)\n",
      "chunk - 1\n",
      "  2.451529 seconds (3.41 M allocations: 104.082 MiB, 2.21% gc time, 18.39% compilation time)\n",
      "chunk - 2\n",
      "  2.466750 seconds (3.34 M allocations: 103.009 MiB, 2.00% gc time, 11.94% compilation time)\n",
      "chunk - 3\n",
      "  2.309540 seconds (3.27 M allocations: 100.052 MiB, 1.69% gc time)\n",
      "chunk - 4\n",
      "  1.775924 seconds (3.27 M allocations: 100.009 MiB, 2.15% gc time)\n",
      "chunk - 5\n",
      "  2.300649 seconds (3.24 M allocations: 99.640 MiB, 1.63% gc time)\n",
      "chunk - 6\n",
      "  1.649341 seconds (3.27 M allocations: 100.000 MiB, 2.03% gc time)\n",
      "chunk - 7\n",
      "  1.860362 seconds (3.22 M allocations: 99.354 MiB, 3.91% gc time, 0.88% compilation time)\n",
      "chunk - 8\n",
      "  1.002840 seconds (3.26 M allocations: 99.890 MiB, 3.53% gc time)\n",
      "chunk - 9\n",
      "  0.071829 seconds (156.58 k allocations: 4.937 MiB, 5.07% gc time)\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 40\n",
    "out = Vector{Any}(undef, length(tpaths))\n",
    "m = Int[]\n",
    "lock_m = ReentrantLock()\n",
    "iii = 0\n",
    "for chunk in Iterators.partition(eachindex(tpaths), chunk_size)\n",
    "    println(\"chunk - $(iii)\")\n",
    "\n",
    "    @time @threads for i in chunk\n",
    "        try\n",
    "            out[i] = sample_tiff_onto_grid(tpaths[i], xs, ys, tf)\n",
    "        catch\n",
    "            lock(lock_m) do\n",
    "                push!(m, i)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    iii = iii + 1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e7f039-fc6a-4855-8542-35e72319661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame()\n",
    "nx = length(xs)\n",
    "ny = length(ys)\n",
    "df.x3035 = repeat(xs, inner=ny)\n",
    "df.y3035 = repeat(ys, outer=nx)\n",
    "for i in eachindex(tnames)\n",
    "    df[!, Symbol(tnames[i])] = out[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "914efeb0-0dd4-418c-9143-d421c88c63e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"overlaid_v20251219.csv\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "CSV.write(\"overlaid_$(version).csv\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6102fd-7194-4959-a821-16b93e7258ec",
   "metadata": {},
   "source": [
    "## prepare the data as how it's processed before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7696b98-7558-4a2c-aace-e021be191843",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV, DataFrames\n",
    "# prepare data\n",
    "# load in preprocessed data to get predictors\n",
    "datafile = \"/mnt/tupi/HybridModeling/EasyDensity.jl/data/lucas_preprocessed_v20251125.csv\"\n",
    "oridf = CSV.read(datafile, DataFrame; normalizenames=true)\n",
    "predictors = Symbol.(names(oridf))[18:end-6]; # CHECK EVERY TIME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04304891-3873-4d3a-8580-1e57ac747382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62577, 422)\n",
      "(62199, 421)\n",
      "(62199, 422)\n",
      "(57343, 422)\n",
      "33487 CHELSA_swe_1981_2010_V_2_1\n",
      "(56117, 415)\n",
      "(56117, 380)\n"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "# ? move the `csv` file into the `BulkDSOC/data` folder (create folder)\n",
    "df_o = CSV.read(\"/mnt/tupi/HybridModeling/EasyDensity.jl/data/lucas_overlaid.csv\", DataFrame, normalizenames=true);\n",
    "println(size(df_o));\n",
    "\n",
    "############################\n",
    "###### clean targets #######\n",
    "############################\n",
    "\n",
    "# filter horizon depth = 10 cm\n",
    "df_o = df_o[df_o.hzn_dep .== 10, :];\n",
    "select!(df_o, Not(:hzn_dep));\n",
    "println(size(df_o))\n",
    "\n",
    "# identify noise time supervise\n",
    "gdf = groupby(df_o, :id);\n",
    "df_o.maxdiff = fill(0.0, nrow(df_o));  # initialize noise column\n",
    "# compute max abs difference of SOCconc per id\n",
    "for sub in groupby(df_o, :id)\n",
    "    soc = sort(sub.soc)\n",
    "\n",
    "    if length(soc) < 2\n",
    "        maxdiff = -1\n",
    "    else\n",
    "        maxdiff = maximum(abs.(diff(soc)))\n",
    "    end\n",
    "\n",
    "    df_o[df_o.id .== sub.id[1], :maxdiff] .= maxdiff\n",
    "    \n",
    "end\n",
    "println(size(df_o))\n",
    "df_o = df_o[df_o.maxdiff .<= 50, :];\n",
    "println(size(df_o))\n",
    "\n",
    "# coords = collect(zip(df_o.lat, df_o.lon));\n",
    "\n",
    "########################\n",
    "###### clean cov #######\n",
    "########################\n",
    "# t clean covariates\n",
    "names_cov = Symbol.(names(df_o))[18:end-1];\n",
    "\n",
    "# Fix soilsuite and cropland extent columns\n",
    "for col in names_cov\n",
    "    if occursin(\"_soilsuite_\", String(col))\n",
    "        df_o[!, col] = replace(df_o[!, col], missing => 0)\n",
    "    elseif occursin(\"cropland_extent_\", String(col))\n",
    "        df_o[!, col] = replace(df_o[!, col], missing => 0)\n",
    "        df_o[!, col] .= ifelse.(df_o[!, col] .> 0, 1, 0)\n",
    "    end\n",
    "end\n",
    "\n",
    "# rm missing values: 1. >5%, drop col; 2. <=5%, drop row\n",
    "cols_to_drop_row = Symbol[];\n",
    "cols_to_drop_col = Symbol[];\n",
    "for col in names_cov\n",
    "    n_missing = count(ismissing, df_o[!, col])\n",
    "    frac_missing = n_missing / nrow(df_o)\n",
    "    if frac_missing > 0.05\n",
    "        println(n_missing, \" \", col)\n",
    "        select!(df_o, Not(col))  # drop the column\n",
    "        push!(cols_to_drop_col, col)  \n",
    "    elseif n_missing > 0\n",
    "        # println(n_missing, \" \", col)\n",
    "        push!(cols_to_drop_row, col)  # collect column name\n",
    "    end\n",
    "\n",
    "    if occursin(\"CHELSA_kg\", String(col)) \n",
    "        push!(cols_to_drop_col, col) \n",
    "        select!(df_o, Not(col))  # rm kg catagorical col\n",
    "    end \n",
    "end\n",
    "\n",
    "names_cov = filter(x -> !(x in cols_to_drop_col), names_cov) # remove cols-to-drop from names_cov\n",
    "if !isempty(cols_to_drop_row) \n",
    "    df_o = subset(df_o, cols_to_drop_row .=> ByRow(!ismissing)) # drop rows with missing values in cols_to_drop_row\n",
    "end\n",
    "println(size(df_o))\n",
    "\n",
    "cols_to_drop_col = Symbol[] \n",
    "for col in names_cov\n",
    "    if std(df_o[:,col])==0\n",
    "        push!(cols_to_drop_col, col)  # rm constant col (std==0)\n",
    "        select!(df_o, Not(col))\n",
    "    end\n",
    "end\n",
    "names_cov = filter(x -> !(x in cols_to_drop_col), names_cov) # remove cols-to-drop from names_cov\n",
    "println(size(df_o))\n",
    "\n",
    "# for col in names_cov # to check covairate distribution\n",
    "#     println(string(col)[1:10], ' ', round(std(df[:, col]); digits=2), ' ', round(mean(df[:, col]); digits=2))\n",
    "# end\n",
    "\n",
    "# # Normalize covariates by (x-mean) / std\n",
    "means = map(c -> mean(skipmissing(df_o[!, c])), predictors)\n",
    "stds  = map(c -> std(skipmissing(df_o[!, c])), predictors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d203830-347a-4823-aae4-ae3371898aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply the normalizations to new training data\n",
    "# get the overlaid data\n",
    "version = \"v20251219\"\n",
    "df = CSV.read(\"overlaid_$(version).csv\", DataFrame)\n",
    "\n",
    "# mend crop and soil suite layers\n",
    "for col in predictors\n",
    "    if occursin(\"_soilsuite_\", String(col))\n",
    "        df[!, col] = replace(df[!, col], missing => 0)\n",
    "    elseif occursin(\"cropland_extent_\", String(col))\n",
    "        df[!, col] = replace(df[!, col], missing => 0)\n",
    "        df[!, col] .= ifelse.(df[!, col] .> 0, 1, 0)\n",
    "    end\n",
    "end\n",
    "\n",
    "for (i, col) in enumerate(predictors)\n",
    "    df[!, col] = (Float64.(df[!, col]) .- means[i]) ./ stds[i]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8436b3d-f18f-44e7-baa7-23f77b10a1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"predictor_quantiles_check.csv\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(x) = filter(!isnan, skipmissing(x))\n",
    "\n",
    "rows = Vector{NamedTuple}()\n",
    "\n",
    "for col in predictors\n",
    "    v_o = clean(oridf[!, col])\n",
    "    v_d = clean(df[!, col])\n",
    "\n",
    "    push!(rows, (\n",
    "        variable   = String(col),\n",
    "        q05_oridf  = quantile(v_o, 0.05),\n",
    "        q05_df     = quantile(v_d, 0.05),\n",
    "        q50_oridf  = quantile(v_o, 0.50),\n",
    "        q50_df     = quantile(v_d, 0.50),\n",
    "        q95_oridf  = quantile(v_o, 0.95),\n",
    "        q95_df     = quantile(v_d, 0.95)\n",
    "    ))\n",
    "end\n",
    "\n",
    "qt = DataFrame(rows)\n",
    "CSV.write(\"predictor_quantiles_check.csv\", qt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1838ad24-a17b-4a79-8ff0-dd24cadb3e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"production_preprocessed_v20251219.csv\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV.write(\"production_preprocessed_$(version).csv\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea42642-14f8-4755-b06a-8f1461861dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.7",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
